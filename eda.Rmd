---
title: "Exploratory Data Analysis"
author: "Robert"
date: "2/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries

```{r, message=FALSE}
library(ggplot2)
library(ggthemes)
library(purrr)
library(dplyr)
library(tidyr)
library(tibble)
library(earth)

source('utils.R', local=knitr::knit_global())
```

## Load Datasets

```{r}
df <- read.csv('data/CaseStudy2-data.csv')
```

## Remove Irrelevant Columns from Data

Some features in the dataset don't have meaningful information for data 
analysis, such as "EmployeeNumber." These columns will be manually removed. 

```{r}
# manually remove columns that don't contain valuable data
pruned_df <- df %>%
  select(-c(EmployeeCount, EmployeeNumber, Over18, StandardHours))
```

## Find Correlations in Dataset

#### Pearson's Correlation Coefficient
The first analysis I will look at is correlations between remaining numerical
features. Pearson correlation assigns a value between -1 and 1, which measures
the strength of the linear relationship between to variables. A Pearson
correlation coefficient (r) of 1 is total positive correlation and -1 is total
negative correlation. 

#### Correlation P-Value
A p-value can also be calculated with the correlation coefficient. The p-value
is the probability that you would have found the current result if the
correlation coefficient were zero. A small p-value signifies a statistically
significant correlation. 


```{r fig.width=10, fig.height=10, warning=FALSE}

# create dataframe of numerical columns
numeric_df <- pruned_df %>% 
  select(where(is.numeric)) %>%
  select(-c(ID))

# create formatted correlation dataframe
corr_df <- format_corr_df(numeric_df, 0.05)

# plot correlation heatmap
corr_df %>%
  ggplot(aes(x = Measure_1, y = Measure_2, fill = r, label=round(r_if_sig,2))) +
  geom_tile() +
  labs(
    x = NULL, 
    y = NULL, 
    fill = "Pearson's\nCorrelation", 
    title="Correlations in Employee Attrition",
    subtitle="Only significant Pearson's correlation coefficients shown") +
  scale_fill_gradient2(mid="#FBFEF9",low="#0C6291",high="#A63446", limits=c(-1,1)) +
  geom_text(size=3) +
  theme_tufte() +
  scale_x_discrete(expand=c(0,0)) +
  scale_y_discrete(expand=c(0,0)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

## Strongest Feature Correlations

The heatmap is valuable for visualizing all pair-wise correlations, but we can
plot the strongest feature correlations on a simple bar chart. 

From the visual, we can see that MonthlyIncome is strongly correlated to both
JobLevel and TotalWorkingYears. Thus, we will remove these from further
analysis.

```{r}
# remove duplicate correlation coefficients
corr_df$Measures <- apply(
  corr_df[,c("Measure_1", "Measure_2")], 1, 
  function(x) paste(sort(x), collapse = "-"))

# visualize top N correlation coefficients
corr_df %>%
  filter(sig_p, !duplicated(Measures)) %>%
  arrange(desc(r)) %>% head(10) %>%
  ggplot(aes(x=r, y=reorder(Measures, r))) + 
  geom_bar(stat='identity', fill='#0C6291', width=0.6) + 
  labs(
    x = "Pearson's Coefficient of Correlation (r)",
    y = NULL,
    title = 'Strongest Correlations in Employee Attrition'
  ) + 
  theme_tufte() 
```
## Remove Strongly Correlated Variables from Dataset

```{r}
pruned_df <- pruned_df %>%
  select(-c(JobLevel, TotalWorkingYears))
```

## Find the Overall Attrition Rate for the Dataset

For further analysis, we need to establish a baseline attrition rate. We can 
calculate this for the sample dataset by simply dividing the attrition by
the total number of observations. 

```{r}
# calculate the percentage total attrition rate for the sample dataset
total_attr_df <- df %>% 
  group_by(Attrition) %>%
  summarise(count = n()) %>%
  mutate(freq = round(count / sum(count), 3))

# save the attrition rate in a variable
total_attr_rate <- total_attr_df[total_attr_df$Attrition == 'Yes', 'freq']

# print the result
paste0('The total attrition rate for the dataset is ', total_attr_rate * 100, '%')
```

##

```{r}
pruned_df %>%
  ggplot(aes(x=MonthlyIncome, color=Attrition)) + 
  geom_density() + 
  geom_vline(
    aes(xintercept=mean(MonthlyIncome)), 
    color="blue", linetype="dashed", size=1) 
```

## Data Preparation 

To prepare the data for model evaluation, I will convert the Attrition values 
into (0, 1) and character vectors into factors.

```{r}
# create a copy of the pruned_df object
clean_df <- data.frame(pruned_df)

# convert the Attrition values to bit
clean_df$Attrition[clean_df$Attrition == 'Yes'] = 1
clean_df$Attrition[clean_df$Attrition == 'No'] = 0

# convert the Attrition data type to numeric
clean_df$Attrition = as.numeric(clean_df$Attrition)

# create vector of factors to convert
vchars <- c(
  'BusinessTravel', 
  'Department',
  'EducationField',
  'Gender',
  'JobRole',
  'MaritalStatus',
  'OverTime')

# convert the character variables into factors
clean_df[, vchars] = lapply(clean_df[, vchars], as.factor)
```

## Feature Selection with Multivariate Adaptive Regression Splines (MARS)

MARS is a non-parametric regression model that automatically models
nonlinearity and interactions between variables. The algorithm achieves this
by creating cut points (knots) similar to step functions. 

MARS models are not sensitive to the scale of the features, so feature 
standardization is not required. 

```{r}
# fit a basic MARS model
mars <- earth(Attrition ~ ., data=clean_df)
```

## Estimating Variable Importance

Variable importance is a measure of the strength of the relationship between 
observed values and the response. 

The `evimp` function uses three criteria for estimating variable importance
in a MARS model. 

1. The `nsubsets` criterion counts the number of model subsets that include the
variable. The subsets are the terms generated by the MARS backward pass. 
Variables that are included in more subsets are considered more important. 

2. The `rss` criterion calculates the decrease in the RSS for each subset
relative to the previous subset during the MARS backward pass. Then, for each
variable it sums the decreases *over all subsets that include that variable*.
Finally, the summed decreases are scaled (largest is 100). 

3. The `gcv` criterion is similar to RSS, but uses Generalized Cross Validation 
(GCV) instead of RSS. 

**Note**: variable importance could be more accurately estimated using a
bagging algorithm. 

```{r}
# estimate the variable importance from the MARS fit
evi <- evimp(mars)

evi_df <- as.data.frame(unclass(evi[, c(3,4,6)])) %>%
  as_tibble(rownames='feature')

# plot the estimated variable importance
evi_df %>%
  ggplot(aes(x=nsubsets, y=reorder(feature, nsubsets))) + 
  geom_bar(stat='identity', fill='#0C6291', width=0.6) + 
  labs(
    x = "Selected Criteria {nsubsets}",
    y = NULL,
    title = 'Estimated Feature Importance in Employee Attrition'
  ) + 
  theme_tufte()   
```
